{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan:\n",
    "\n",
    "- ~~Извлекать статьи по ключевым словам;~~\n",
    "- ~~Суммаризировать каждую статью;~~\n",
    "- Суммаризация подходов из статей;\n",
    "- Рерайтинг запроса;\n",
    "- Статья в виде json;\n",
    "- ~~Выбор подходящей LLM;~~ (https://ollama.com/library/llama3.1:8b-instruct-q6_K)\n",
    "- Разработка веб-интерфейса;\n",
    "- *Попробовать RAPTOR для суммаризации (https://arxiv.org/html/2401.18059v1);\n",
    "\n",
    "Для работы с Ollama - https://github.com/ollama/ollama\n",
    "\n",
    "Гайд по разработке ReAct Агента - https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/#create-react-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph ReAct agent tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "import arxiv\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1:8b-instruct-q6_K\")\n",
    "\n",
    "# Инструмент поиска на Arxiv\n",
    "@tool(\"ArxivSearch\") # name=\"arxiv_search\", description=\"\"\n",
    "def arxiv_search_tool(query: str, max_results: int = 2):\n",
    "    \"\"\"Search articles on Arxiv by keywords\"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    articles = []\n",
    "    for result in search.results():\n",
    "        articles.append({\n",
    "            \"title\": result.title,\n",
    "            \"summary\": result.summary,\n",
    "            \"url\": result.entry_id\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "# Инструмент суммаризации текста\n",
    "@tool(\"SummarizingTool\") # name=\"summarize_tool\", description=\"Summarizes text using a language model\"\n",
    "def summarize_tool(text: str):\n",
    "    \"\"\"Summarizes text using a language model\"\"\"\n",
    "    prompt = f\"Summarize the following article: {text}\"\n",
    "    summary = model(prompt)\n",
    "    return summary\n",
    "\n",
    "\n",
    "tools = [arxiv_search_tool, summarize_tool]\n",
    "model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "def tool_node(state: AgentState):\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "):\n",
    "    system_prompt = SystemMessage(\n",
    "        \"You are a helpful AI assistant that takes a user input and summarize arxiv articles found by keywords from input. You can use tool you have for searching articles and summarizing them.\"\n",
    "        )\n",
    "    response = model.invoke([system_prompt] + state[\"messages\"], config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== HumanMessage ===================\n",
      "attention\n",
      "=================== AIMessage ===================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koltu\\AppData\\Local\\Temp\\ipykernel_12644\\477668848.py:16: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== ToolMessage ===================\n",
      "[{\"title\": \"Attention and Self-Attention in Random Forests\", \"summary\": \"New models of random forests jointly using the attention and self-attention\\nmechanisms are proposed for solving the regression problem. The models can be\\nregarded as extensions of the attention-based random forest whose idea stems\\nfrom applying a combination of the Nadaraya-Watson kernel regression and the\\nHuber's contamination model to random forests. The self-attention aims to\\ncapture dependencies of the tree predictions and to remove noise or anomalous\\npredictions in the random forest. The self-attention module is trained jointly\\nwith the attention module for computing weights. It is shown that the training\\nprocess of attention weights is reduced to solving a single quadratic or linear\\noptimization problem. Three modifications of the general approach are proposed\\nand compared. A specific multi-head self-attention for the random forest is\\nalso considered. Heads of the self-attention are obtained by changing its\\ntuning parameters including the kernel parameters and the contamination\\nparameter of models. Numerical experiments with various datasets illustrate the\\nproposed models and show that the supplement of the self-attention improves the\\nmodel performance for many datasets.\", \"url\": \"http://arxiv.org/abs/2207.04293v1\"}, {\"title\": \"Attention in Reasoning: Dataset, Analysis, and Modeling\", \"summary\": \"While attention has been an increasingly popular component in deep neural\\nnetworks to both interpret and boost the performance of models, little work has\\nexamined how attention progresses to accomplish a task and whether it is\\nreasonable. In this work, we propose an Attention with Reasoning capability\\n(AiR) framework that uses attention to understand and improve the process\\nleading to task outcomes. We first define an evaluation metric based on a\\nsequence of atomic reasoning operations, enabling a quantitative measurement of\\nattention that considers the reasoning process. We then collect human\\neye-tracking and answer correctness data, and analyze various machine and human\\nattention mechanisms on their reasoning capability and how they impact task\\nperformance. To improve the attention and reasoning ability of visual question\\nanswering models, we propose to supervise the learning of attention\\nprogressively along the reasoning process and to differentiate the correct and\\nincorrect attention patterns. We demonstrate the effectiveness of the proposed\\nframework in analyzing and modeling attention with better reasoning capability\\nand task performance. The code and data are available at\\nhttps://github.com/szzexpoi/AiR\", \"url\": \"http://arxiv.org/abs/2204.09774v1\"}, {\"title\": \"Multi-stage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images\", \"summary\": \"The attention mechanism can refine the extracted feature maps and boost the\\nclassification performance of the deep network, which has become an essential\\ntechnique in computer vision and natural language processing. However, the\\nmemory and computational costs of the dot-product attention mechanism increase\\nquadratically with the spatio-temporal size of the input. Such growth hinders\\nthe usage of attention mechanisms considerably in application scenarios with\\nlarge-scale inputs. In this Letter, we propose a Linear Attention Mechanism\\n(LAM) to address this issue, which is approximately equivalent to dot-product\\nattention with computational efficiency. Such a design makes the incorporation\\nbetween attention mechanisms and deep networks much more flexible and\\nversatile. Based on the proposed LAM, we re-factor the skip connections in the\\nraw U-Net and design a Multi-stage Attention ResU-Net (MAResU-Net) for semantic\\nsegmentation from fine-resolution remote sensing images. Experiments conducted\\non the Vaihingen dataset demonstrated the effectiveness and efficiency of our\\nMAResU-Net. Open-source code is available at\\nhttps://github.com/lironui/Multistage-Attention-ResU-Net.\", \"url\": \"http://arxiv.org/abs/2011.14302v2\"}, {\"title\": \"Attention cannot be an Explanation\", \"summary\": \"Attention based explanations (viz. saliency maps), by providing\\ninterpretability to black box models such as deep neural networks, are assumed\\nto improve human trust and reliance in the underlying models. Recently, it has\\nbeen shown that attention weights are frequently uncorrelated with\\ngradient-based measures of feature importance. Motivated by this, we ask a\\nfollow-up question: \\\"Assuming that we only consider the tasks where attention\\nweights correlate well with feature importance, how effective are these\\nattention based explanations in increasing human trust and reliance in the\\nunderlying models?\\\". In other words, can we use attention as an explanation? We\\nperform extensive human study experiments that aim to qualitatively and\\nquantitatively assess the degree to which attention based explanations are\\nsuitable in increasing human trust and reliance. Our experiment results show\\nthat attention cannot be used as an explanation.\", \"url\": \"http://arxiv.org/abs/2201.11194v1\"}, {\"title\": \"Parallel Attention Forcing for Machine Translation\", \"summary\": \"Attention-based autoregressive models have achieved state-of-the-art\\nperformance in various sequence-to-sequence tasks, including Text-To-Speech\\n(TTS) and Neural Machine Translation (NMT), but can be difficult to train. The\\nstandard training approach, teacher forcing, guides a model with the reference\\nback-history. During inference, the generated back-history must be used. This\\nmismatch limits the evaluation performance. Attention forcing has been\\nintroduced to address the mismatch, guiding the model with the generated\\nback-history and reference attention. While successful in tasks with continuous\\noutputs like TTS, attention forcing faces additional challenges in tasks with\\ndiscrete outputs like NMT. This paper introduces the two extensions of\\nattention forcing to tackle these challenges. (1) Scheduled attention forcing\\nautomatically turns attention forcing on and off, which is essential for tasks\\nwith discrete outputs. (2) Parallel attention forcing makes training parallel,\\nand is applicable to Transformer-based models. The experiments show that the\\nproposed approaches improve the performance of models based on RNNs and\\nTransformers.\", \"url\": \"http://arxiv.org/abs/2211.03237v1\"}, {\"title\": \"Neural Attention for Image Captioning: Review of Outstanding Methods\", \"summary\": \"Image captioning is the task of automatically generating sentences that\\ndescribe an input image in the best way possible. The most successful\\ntechniques for automatically generating image captions have recently used\\nattentive deep learning models. There are variations in the way deep learning\\nmodels with attention are designed. In this survey, we provide a review of\\nliterature related to attentive deep learning models for image captioning.\\nInstead of offering a comprehensive review of all prior work on deep image\\ncaptioning models, we explain various types of attention mechanisms used for\\nthe task of image captioning in deep learning models. The most successful deep\\nlearning models used for image captioning follow the encoder-decoder\\narchitecture, although there are differences in the way these models employ\\nattention mechanisms. Via analysis on performance results from different\\nattentive deep models for image captioning, we aim at finding the most\\nsuccessful types of attention mechanisms in deep models for image captioning.\\nSoft attention, bottom-up attention, and multi-head attention are the types of\\nattention mechanism widely used in state-of-the-art attentive deep learning\\nmodels for image captioning. At the current time, the best results are achieved\\nfrom variants of multi-head attention with bottom-up attention.\", \"url\": \"http://arxiv.org/abs/2111.15015v1\"}, {\"title\": \"Salience Estimation with Multi-Attention Learning for Abstractive Text Summarization\", \"summary\": \"Attention mechanism plays a dominant role in the sequence generation models\\nand has been used to improve the performance of machine translation and\\nabstractive text summarization. Different from neural machine translation, in\\nthe task of text summarization, salience estimation for words, phrases or\\nsentences is a critical component, since the output summary is a distillation\\nof the input text. Although the typical attention mechanism can conduct text\\nfragment selection from the input text conditioned on the decoder states, there\\nis still a gap to conduct direct and effective salience detection. To bring\\nback direct salience estimation for summarization with neural networks, we\\npropose a Multi-Attention Learning framework which contains two new attention\\nlearning components for salience estimation: supervised attention learning and\\nunsupervised attention learning. We regard the attention weights as the\\nsalience information, which means that the semantic units with large attention\\nvalue will be more important. The context information obtained based on the\\nestimated salience is incorporated with the typical attention mechanism in the\\ndecoder to conduct summary generation. Extensive experiments on some benchmark\\ndatasets in different languages demonstrate the effectiveness of the proposed\\nframework for the task of abstractive summarization.\", \"url\": \"http://arxiv.org/abs/2004.03589v1\"}, {\"title\": \"Human Action Recognition: Pose-based Attention draws focus to Hands\", \"summary\": \"We propose a new spatio-temporal attention based mechanism for human action\\nrecognition able to automatically attend to the hands most involved into the\\nstudied action and detect the most discriminative moments in an action.\\nAttention is handled in a recurrent manner employing Recurrent Neural Network\\n(RNN) and is fully-differentiable. In contrast to standard soft-attention based\\nmechanisms, our approach does not use the hidden RNN state as input to the\\nattention model. Instead, attention distributions are extracted using external\\ninformation: human articulated pose. We performed an extensive ablation study\\nto show the strengths of this approach and we particularly studied the\\nconditioning aspect of the attention mechanism. We evaluate the method on the\\nlargest currently available human action recognition dataset, NTU-RGB+D, and\\nreport state-of-the-art results. Other advantages of our model are certain\\naspects of explanability, as the spatial and temporal attention distributions\\nat test time allow to study and verify on which parts of the input data the\\nmethod focuses.\", \"url\": \"http://arxiv.org/abs/1712.08002v1\"}, {\"title\": \"Intentional Attention Mask Transformation for Robust CNN Classification\", \"summary\": \"Convolutional Neural Networks have achieved impressive results in various\\ntasks, but interpreting the internal mechanism is a challenging problem. To\\ntackle this problem, we exploit a multi-channel attention mechanism in feature\\nspace. Our network architecture allows us to obtain an attention mask for each\\nfeature while existing CNN visualization methods provide only a common\\nattention mask for all features. We apply the proposed multi-channel attention\\nmechanism to multi-attribute recognition task. We can obtain different\\nattention mask for each feature and for each attribute. Those analyses give us\\ndeeper insight into the feature space of CNNs. Furthermore, our proposed\\nattention mechanism naturally derives a method for improving the robustness of\\nCNNs. From the observation of feature space based on the proposed attention\\nmask, we demonstrate that we can obtain robust CNNs by intentionally\\nemphasizing features that are important for attributes. The experimental\\nresults for the benchmark dataset show that the proposed method gives high\\nhuman interpretability while accurately grasping the attributes of the data,\\nand improves network robustness.\", \"url\": \"http://arxiv.org/abs/1905.02719v2\"}, {\"title\": \"Multi-Branch with Attention Network for Hand-Based Person Recognition\", \"summary\": \"In this paper, we propose a novel hand-based person recognition method for\\nthe purpose of criminal investigations since the hand image is often the only\\navailable information in cases of serious crime such as sexual abuse. Our\\nproposed method, Multi-Branch with Attention Network (MBA-Net), incorporates\\nboth channel and spatial attention modules in branches in addition to a global\\n(without attention) branch to capture global structural information for\\ndiscriminative feature learning. The attention modules focus on the relevant\\nfeatures of the hand image while suppressing the irrelevant backgrounds. In\\norder to overcome the weakness of the attention mechanisms, equivariant to\\npixel shuffling, we integrate relative positional encodings into the spatial\\nattention module to capture the spatial positions of pixels. Extensive\\nevaluations on two large multi-ethnic and publicly available hand datasets\\ndemonstrate that our proposed method achieves state-of-the-art performance,\\nsurpassing the existing hand-based identification methods.\", \"url\": \"http://arxiv.org/abs/2108.02234v5\"}]\n",
      "=================== AIMessage ===================\n",
      "The papers listed are related to attention mechanisms in deep learning, which is a technique used to focus on specific parts of the input data that are relevant for the task at hand. Here's a brief summary of each paper:\n",
      "\n",
      "1. **Human Action Recognition: Pose-based Attention draws focus to Hands** - This paper proposes a new spatio-temporal attention mechanism for human action recognition that focuses on the hands involved in an action.\n",
      "2. **Intentional Attention Mask Transformation for Robust CNN Classification** - This paper introduces a multi-channel attention mechanism that allows for different attention masks for each feature, providing deeper insight into the feature space of CNNs and improving robustness.\n",
      "3. **Multi-Branch with Attention Network for Hand-Based Person Recognition** - This paper proposes a novel hand-based person recognition method that incorporates channel and spatial attention modules to capture global structural information and focus on relevant features.\n",
      "\n",
      "The other papers are related to various applications of attention mechanisms, including:\n",
      "\n",
      "* **Human Action Recognition: Pose-based Attention draws focus to Hands**: This paper also evaluates the method on the largest currently available human action recognition dataset, NTU-RGB+D, and reports state-of-the-art results.\n",
      "* **Intentional Attention Mask Transformation for Robust CNN Classification**: This paper demonstrates that the proposed attention mechanism naturally derives a method for improving the robustness of CNNs by intentionally emphasizing features important for attributes.\n",
      "* **Multi-Branch with Attention Network for Hand-Based Person Recognition**: This paper also evaluates its performance on two large multi-ethnic and publicly available hand datasets, demonstrating state-of-the-art results.\n",
      "\n",
      "The other papers are more general applications of attention mechanisms in deep learning, including:\n",
      "\n",
      "* **Human Action Recognition: Pose-based Attention draws focus to Hands**: This paper performs an extensive ablation study to show the strengths of this approach and studies the conditioning aspect of the attention mechanism.\n",
      "* **Intentional Attention Mask Transformation for Robust CNN Classification**: This paper also provides a novel way of visualizing feature importance in CNNs using the proposed multi-channel attention mechanism.\n",
      "* **Multi-Branch with Attention Network for Hand-Based Person Recognition**: This paper demonstrates that the proposed method gives high human interpretability while accurately grasping the attributes of the data, and improves network robustness.\n",
      "\n",
      "Overall, these papers demonstrate the effectiveness of attention mechanisms in various applications, from human action recognition to hand-based person recognition.\n"
     ]
    }
   ],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, BaseMessage):\n",
    "            print(f\"=================== {message.__class__.__name__} ===================\")\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "        print(message.content)\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"attention\")]}\n",
    "print_stream(graph.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
